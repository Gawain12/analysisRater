import requests
import csv
import time
import json
import os
import re
import sys 
from datetime import datetime
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config.config import IMDB_CONFIG

class CurlParser:
    """ä¸€ä¸ªç”¨äºè§£æcURLå‘½ä»¤å­—ç¬¦ä¸²ä»¥æå–å…³é”®ä¿¡æ¯çš„å·¥å…·ç±»ã€‚"""
    @staticmethod
    def parse(curl_string):
        headers = {};
        try:
            curl_string = curl_string.replace('^', '').replace('\\\n', ' ').replace('`\n', ' ')
            header_matches = re.findall(r"-H '([^']*)'|--header '([^']*)'|\-H \"([^\"]*)\"", curl_string)
            for header_line in [next(item for item in match if item) for match in header_matches]:
                if ':' in header_line: key, value = header_line.split(':', 1); headers[key.strip().lower()] = value.strip()
            cookie_match = re.search(r"--cookie '([^']*)'|--cookie \"([^\"]*)\"", curl_string)
            if cookie_match: headers['cookie'] = next(c for c in cookie_match.groups() if c)
            if 'cookie' not in headers: raise ValueError("cURLå‘½ä»¤ä¸­ç¼ºå°‘å…³é”®çš„ 'cookie' ä¿¡æ¯ã€‚")
            return headers
        except Exception as e: print(f"âŒ cURLè§£æå¤±è´¥: {e}"); return None

class IMDbRatingsScraper:
    """
    é€šè¿‡äº¤å‰è·å–ç§»åŠ¨ç«¯APIå’Œç½‘é¡µç«¯æ•°æ®ï¼Œå…¨é¢ã€é«˜æ•ˆåœ°æŠ“å–IMDbç”¨æˆ·è¯„åˆ†ã€‚
    V-Final-Incremental: æ”¯æŒå¢é‡æ›´æ–°ï¼Œæå¤§æå‡åç»­è¿è¡Œé€Ÿåº¦ã€‚
    """
    AUTH_FILE = 'auth.json'; CURL_FILE = 'curl_command.txt'
    EXPORT_FILENAME = 'imdb_ratings_export.csv'

    def __init__(self):
        self.user_id = IMDB_CONFIG.get('user_id')
        if not self.user_id: raise ValueError("IMDB_CONFIGä¸­çš„'user_id'æœªè®¾ç½®ã€‚")
        self.api_base_url = "https://api.graphql.imdb.com/"
        self.web_base_url = f"https://www.imdb.com/user/{self.user_id}/ratings"
        self.session = requests.Session()
        
        self.api_headers = self._authenticate()
        self.web_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
        }

    def _authenticate(self):
        # (Authentication logic is unchanged)
        if os.path.exists(self.AUTH_FILE):
            try:
                with open(self.AUTH_FILE, 'r', encoding='utf-8') as f: headers = json.load(f)
                print("âœ… å·²ä» `auth.json` æ–‡ä»¶æˆåŠŸåŠ è½½è®¤è¯ä¿¡æ¯ã€‚")
                return headers
            except (json.JSONDecodeError, KeyError): print(f"âš ï¸ `{self.AUTH_FILE}` æ–‡ä»¶å·²æŸå...")
        if os.path.exists(self.CURL_FILE) and os.path.getsize(self.CURL_FILE) > 0:
            print(f"â„¹ï¸ æ£€æµ‹åˆ° `{self.CURL_FILE}` æ–‡ä»¶ï¼Œæ­£åœ¨å°è¯•è§£æ...")
            with open(self.CURL_FILE, 'r', encoding='utf-8') as f: curl_string = f.read()
            headers = CurlParser.parse(curl_string)
            if headers:
                with open(self.AUTH_FILE, 'w', encoding='utf-8') as f: json.dump(headers, f, indent=2)
                processed_filename = f"{self.CURL_FILE}.processed_on_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(self.CURL_FILE, processed_filename)
                print("\nâœ… è§£ææˆåŠŸï¼è®¤è¯ä¿¡æ¯å·²ä¿å­˜è‡³ `auth.json`ã€‚")
                return headers
            else: raise SystemExit("âŒ è§£æå¤±è´¥ã€‚")
        else:
            print("\n" + "="*60 + "\nâš™ï¸ é¦–æ¬¡è¿è¡Œæˆ–éœ€è¦é‡æ–°è®¤è¯ã€‚")
            with open(self.CURL_FILE, 'w') as f: pass
            print(f"ğŸ’¡ æˆ‘å·²åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºäº†ä¸€ä¸ªæ–‡ä»¶: '{self.CURL_FILE}'")
            print(f"   è¯·å°†è·å–çš„cURLå‘½ä»¤ã€ç²˜è´´åˆ° '{self.CURL_FILE}' æ–‡ä»¶ä¸­å¹¶ä¿å­˜ã€‘ï¼Œç„¶åã€é‡æ–°è¿è¡Œã€‘æ­¤è„šæœ¬ã€‚")
            raise SystemExit("="*60)

    def _fetch_api_page(self, cursor):
        # (Unchanged)
        payload = {"operationName": "userRatings", "variables": {"first": 250},"extensions": {"persistedQuery": {"version": 1, "sha256Hash": "ebf2387fd2ba45d62fc54ed2ffe3940086af52e700a1b3929a099d5fce23330a"}}}
        if cursor: payload['variables']['after'] = cursor
        try:
            response = self.session.post(self.api_base_url, json=payload, headers=self.api_headers, timeout=30)
            response.raise_for_status(); return response.json()
        except requests.RequestException as e: print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}"); return None

    def _fetch_web_page(self, page_num):
        # (Unchanged)
        url = f"{self.web_base_url}?sort=date_added,desc&page={page_num}"
        try:
            response = self.session.get(url, headers=self.web_headers, timeout=30)
            response.raise_for_status(); return response.text
        except requests.RequestException as e: print(f"âŒ ç½‘é¡µè¯·æ±‚å¤±è´¥: {e}"); return None

    def _parse_movie_details_from_node(self, node):
        # (Unchanged, already safe and rich)
        details = {}; title_info = node.get('title', {}) or {}
        details['imdb_id'] = title_info.get('id');
        if not details['imdb_id']: return None
        details['title'] = (title_info.get('titleText', {}) or {}).get('text'); details['year'] = (title_info.get('releaseYear', {}) or {}).get('year')
        details['title_type'] = (title_info.get('titleType', {}) or {}).get('text')
        release_date_obj = title_info.get('releaseDate', {}) or {}
        if all(k in release_date_obj for k in ['year', 'month', 'day']):
            try: details['release_date'] = f"{release_date_obj['year']}-{str(release_date_obj['month']).zfill(2)}-{str(release_date_obj['day']).zfill(2)}"
            except TypeError: details['release_date'] = None
        else: details['release_date'] = None
        ratings_summary = title_info.get('ratingsSummary', {}) or {}; details['imdb_rating'] = ratings_summary.get('aggregateRating'); details['imdb_votes'] = ratings_summary.get('voteCount')
        runtime = title_info.get('runtime', {}) or {}; details['runtime_minutes'] = (runtime.get('seconds', 0) or 0) // 60
        details['genres'] = ', '.join([g.get('genre', {}).get('text', '') for g in (title_info.get('titleGenres', {}) or {}).get('genres', []) or []])
        credits_list = title_info.get('principalCredits', []) or []; directors = []
        for credit_category in credits_list:
            if (credit_category.get('category', {}) or {}).get('id') == 'director':
                directors.extend([(c.get('name', {}) or {}).get('nameText', {}).get('text', '') for c in credit_category.get('credits', []) or []])
        details['director'] = ', '.join(directors)
        return details

    def scrape_interleaved(self, existing_rating_ids=None):
        """ä»¥äº¤å‰æ¨¡å¼æŠ“å–å¹¶åˆå¹¶æ•°æ®ï¼Œæ”¯æŒå¢é‡æ›´æ–°ã€‚"""
        if existing_rating_ids is None: existing_rating_ids = set()
        print(f"\nğŸš€ å¼€å§‹æŠ“å–æ•°æ®... (æ¨¡å¼: {'å¢é‡æ›´æ–°' if existing_rating_ids else 'é¦–æ¬¡å®Œæ•´æ‰«æ'})")
        
        newly_scraped_movies = []
        stop_scraping = False
        page_num, api_cursor, web_page = 1, None, 1; api_done, web_done = False, False

        while not stop_scraping and not (api_done and web_done):
            print(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ ---")
            
            personal_data_map_page = {}
            if not api_done:
                print("  - [API]  æ­£åœ¨è¯·æ±‚æ‚¨çš„ä¸ªäººè¯„åˆ†...")
                # ... (API logic, unchanged)
                api_data = self._fetch_api_page(api_cursor)
                if api_data and not api_data.get("errors"):
                    ratings_data = api_data.get('data', {}).get('userRatings')
                    if ratings_data and ratings_data.get('edges'):
                        edges = ratings_data.get('edges', [])
                        for edge in edges:
                            node = edge.get('node', {}); imdb_id = node.get('title', {}).get('id')
                            if not imdb_id: continue
                            ur = node.get('userRating', {})
                            rating_date_raw = ur.get('date')
                            personal_data_map_page[imdb_id] = {
                                'my_rating': ur.get('value'),
                                'rating_date': datetime.fromisoformat(rating_date_raw.replace('Z', '+00:00')).strftime('%Y-%m-%d') if rating_date_raw else None
                            }
                        print(f"  - [API]  å·²è·å– {len(edges)} æ¡ä¸ªäººè¯„åˆ†æ•°æ®ã€‚")
                        page_info = ratings_data.get('pageInfo', {}); api_cursor = page_info.get('endCursor')
                        if not page_info.get('hasNextPage', False): api_done = True; print("  - [API]  æ‚¨çš„æ‰€æœ‰ä¸ªäººè¯„åˆ†å·²è·å–å®Œæ¯•ã€‚")
                    else: api_done = True; print("  - [API]  æœªæ‰¾åˆ°æ›´å¤šè¯„åˆ†æ•°æ®ã€‚")
                else: api_done = True; print(f"  - [API]  è¯·æ±‚å¤±è´¥æˆ–è¿”å›é”™è¯¯ã€‚")

            if not web_done and not stop_scraping:
                print("  - [Web]  æ­£åœ¨æ‰¹é‡è·å–ç”µå½±å…¬å¼€è¯¦æƒ…...")
                # ... (Web logic, unchanged)
                html_content = self._fetch_web_page(web_page)
                if html_content:
                    match = re.search(r'<script id="__NEXT_DATA__" type="application/json">(.+?)</script>', html_content, re.DOTALL)
                    if match:
                        data = json.loads(match.group(1)); search_results = data['props']['pageProps']['mainColumnData']['advancedTitleSearch']
                        movies_on_page = [self._parse_movie_details_from_node(edge.get('node', {})) for edge in search_results.get('edges', [])]
                        movies_on_page = [m for m in movies_on_page if m]
                        
                        movies_to_add_this_page = []
                        for movie in movies_on_page:
                            # --- å¢é‡æ›´æ–°æ ¸å¿ƒé€»è¾‘ ---
                            if movie['imdb_id'] in existing_rating_ids:
                                print(f"  - [INC]  å‘ç°å·²å­˜åœ¨çš„ç”µå½± '{movie['title']}' (ID: {movie['imdb_id']})ã€‚åœæ­¢æ‰«æã€‚")
                                stop_scraping = True
                                break # åœæ­¢å¤„ç†æœ¬é¡µçš„åç»­ç”µå½±
                            
                            if movie['imdb_id'] in personal_data_map_page:
                                movie.update(personal_data_map_page[movie['imdb_id']])
                            movies_to_add_this_page.append(movie)
                        
                        newly_scraped_movies.extend(movies_to_add_this_page)
                        print(f"  - [Merge] æœ¬é¡µæ–°å¢äº† {len(movies_to_add_this_page)} æ¡è®°å½•ã€‚")
                        
                        if not search_results.get('pageInfo', {}).get('hasNextPage', False): web_done = True; print("  - [Web]  æ‰€æœ‰å…¬å¼€ç”µå½±é¡µé¢å·²è·å–å®Œæ¯•ã€‚")
                    else: web_done = True; print("  - [Web]  é¡µé¢ç»“æ„å˜åŒ–ï¼Œæœªæ‰¾åˆ° __NEXT_DATA__ã€‚")
                else: web_done = True; print("  - [Web]  è¯·æ±‚å¤±è´¥ã€‚")
            
            page_num += 1; web_page += 1
            if not stop_scraping and not (api_done and web_done): time.sleep(1.0)
        
        return newly_scraped_movies


def main():
    start_time = time.time()
    print("="*60 + "\n" + " IMDb è¯„åˆ†å¯¼å‡ºå·¥å…· V-Final (å¢é‡æ›´æ–°)".center(66) + "\n" + "="*60)
    try:
        scraper = IMDbRatingsScraper()
        
        # Define the output path in the parent directory
        output_filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', f"imdb_{scraper.user_id}_ratings.csv")
        
        # Step 1: Load existing data for incremental update
        existing_movies = []
        existing_rating_ids = set()
        if os.path.exists(output_filename):
            try:
                with open(output_filename, 'r', newline='', encoding='utf-8') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row in reader:
                        existing_movies.append(row)
                        if row.get('Const'):
                            existing_rating_ids.add(row['Const'])
                print(f"âœ… æˆåŠŸåŠ è½½äº† {len(existing_movies)} æ¡å·²å­˜åœ¨çš„è®°å½•ã€‚")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰CSVæ–‡ä»¶ '{output_filename}' æ—¶å‡ºé”™: {e}ã€‚å°†æ‰§è¡Œå®Œæ•´æ‰«æã€‚")

        # Step 2: Run the scraper with the set of existing IDs
        newly_scraped_movies = scraper.scrape_interleaved(existing_rating_ids)
        
        # Step 3: Save results
        if newly_scraped_movies:
            print(f"\nâœ… å¢é‡æ›´æ–°å®Œæˆï¼Œå…±æ‰¾åˆ° {len(newly_scraped_movies)} æ¡æ–°è®°å½•ã€‚")
            
            export_fieldnames = ['Const', 'Your Rating', 'Date Rated', 'Title', 'URL', 'Title Type', 'IMDb Rating', 'Runtime (mins)', 'Year', 'Genres', 'Num Votes', 'Release Date', 'Directors']
            
            # Convert newly scraped data to the export format
            new_export_data = []
            for movie in newly_scraped_movies:
                new_export_data.append({
                    'Const': movie.get('imdb_id'), 'Your Rating': movie.get('my_rating'), 'Date Rated': movie.get('rating_date'),
                    'Title': movie.get('title'), 'URL': f"https://www.imdb.com/title/{movie.get('imdb_id')}/",
                    'Title Type': movie.get('title_type'), 'IMDb Rating': movie.get('imdb_rating'), 'Runtime (mins)': movie.get('runtime_minutes'),
                    'Year': movie.get('year'), 'Genres': movie.get('genres'), 'Num Votes': movie.get('imdb_votes'),
                    'Release Date': movie.get('release_date'), 'Directors': movie.get('director')
                })
            
            # Combine new data with existing data (newest first)
            combined_data = new_export_data + existing_movies
            
            with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=export_fieldnames, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(combined_data)
            print(f"ğŸ‰ å®Œç¾ï¼å·²å°† {len(combined_data)} æ¡å®Œæ•´è®°å½•ä»¥æ ‡å‡†æ ¼å¼ä¿å­˜åˆ°æ–‡ä»¶: {output_filename}")
        else:
            print("\nâœ… æ‚¨çš„æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")

    except SystemExit as e:
        print(f"\nè„šæœ¬å·²ç»ˆæ­¢: {e}")
    except Exception as e:
        import traceback; print(f"\nå‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e}"); traceback.print_exc()
    finally:
        duration = time.time() - start_time
        if duration > 1.0:
            print("\n" + "="*60 + f"\nç¨‹åºè¿è¡Œå®Œæ¯•ï¼Œæ€»è€—æ—¶: {duration:.2f} ç§’ã€‚\n" + "="*60)

if __name__ == "__main__":
    main()
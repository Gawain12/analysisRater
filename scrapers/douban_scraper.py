import asyncio
import os
import random
import re
import time
import urllib.parse
import sys
import csv
from typing import Union

# Add parent directory to path to allow importing 'config'
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import aiohttp
import pandas as pd
from tqdm.asyncio import tqdm

# ==============================================================================
# Part 1: Web Parsing Logic (formerly web_parser.py)
# ==============================================================================

BROWSER_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

async def fetch_imdb_id_from_web(session: aiohttp.ClientSession, douban_url: str, retries=3) -> Union[str, None]:
    """
    Asynchronously fetches a Douban movie page and parses the IMDB ID from its HTML content.
    It uses the headers pre-configured in the session.
    """
    if not douban_url:
        return None
    
    # Do not print for validation call to keep output clean
    if "1298697" not in douban_url:
        print(f"  - [Web Fetch] Cache miss, fetching IMDB ID from: {douban_url}")
    
    for attempt in range(retries):
        await asyncio.sleep(random.uniform(0.5, 1.5))
        try:
            # The session now has the headers, so no need to pass them here.
            async with session.get(douban_url, verify_ssl=False, timeout=30) as response:
                if response.status == 404:
                    return None
                response.raise_for_status()
                html_content = await response.text()
                imdb_match = re.search(r'IMDb:</span>\s*(tt\d+)', html_content)
                if imdb_match:
                    return imdb_match.group(1)
                return None
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            # Suppress error for validation call, but show for others
            if "1298697" not in douban_url:
                print(f"âŒ Web fetch failed (URL={douban_url}, attempt {attempt + 1}/{retries}): {e}")
            if attempt + 1 == retries and "1298697" not in douban_url:
                print(f"âŒ Max retries reached, giving up on {douban_url}")
            await asyncio.sleep(3)
    return None

# ==============================================================================
# Part 2: Main Scraper Application
# ==============================================================================

# --- Configuration ---
from config.config import DOUBAN_CONFIG

DOUBAN_USER_ID = DOUBAN_CONFIG.get('user')
DOUBAN_HEADERS = DOUBAN_CONFIG.get('headers', {})

if not DOUBAN_USER_ID or not DOUBAN_HEADERS.get('Cookie'):
    raise SystemExit(
        "âŒ é…ç½®é”™è¯¯: è¯·ç¡®ä¿åœ¨ `config.py` çš„ `DOUBAN_CONFIG` ä¸­æä¾›äº† 'user' å’Œ 'Cookie'ã€‚"
    )

print("âœ… å·²ä» `config.py` åŠ è½½ Douban è®¤è¯ä¿¡æ¯ã€‚")


# --- API ---
LIST_API_URL = f"https://m.douban.com/rexxar/api/v2/user/{DOUBAN_USER_ID}/interests"

# --- Validation ---
async def validate_cookie(session):
    """
    Validates the Douban cookie by using the core 'fetch_imdb_id_from_web'
    function. This is the most reliable validation method as it directly
    tests the functionality that requires a valid session.
    """
    print("\nğŸ” æ­£åœ¨éªŒè¯ Douban Cookie çš„æœ‰æ•ˆæ€§...")
    test_douban_url = "https://m.douban.com/movie/subject/1298697/"
    validation_id = await fetch_imdb_id_from_web(session, test_douban_url)
    
    if validation_id:
        print(f"âœ… Douban Cookie éªŒè¯é€šè¿‡ (è·å–åˆ°æµ‹è¯• ID: {validation_id})ã€‚")
        return True
    else:
        print("âŒ Cookie æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚æ— æ³•è·å–æµ‹è¯•é¡µé¢çš„ IMDb IDã€‚")
        return False

# --- ç¼“å­˜æ–‡ä»¶ ---
IMDB_CACHE_FILE = "db_imdb.csv"

def load_imdb_cache():
    """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½è±†ç“£IDåˆ°IMDB IDçš„æ˜ å°„ã€‚"""
    if not os.path.exists(IMDB_CACHE_FILE):
        return {}
    try:
        df = pd.read_csv(IMDB_CACHE_FILE, dtype={'id': str, 'imdb': str})
        if df.empty or not {'id', 'imdb'}.issubset(df.columns):
            return {}
        df.drop_duplicates(subset=['id'], keep='last', inplace=True)
        return pd.Series(df.imdb.values, index=df.id).to_dict()
    except (pd.errors.ParserError, pd.errors.EmptyDataError, KeyError) as e:
        print(f"âš ï¸ Cache file '{IMDB_CACHE_FILE}' is corrupted or invalid. Deleting it and starting fresh. Reason: {e}")
        try:
            os.remove(IMDB_CACHE_FILE)
        except OSError as remove_err:
            print(f"âŒ Failed to delete corrupted cache file: {remove_err}")
        return {}

def save_imdb_cache(imdb_cache: dict):
    """Saves the entire ID cache to the file, overwriting it to ensure integrity."""
    if not imdb_cache:
        return
    
    print(f"\nâœï¸  Saving {len(imdb_cache)} total entries to the IMDB cache file (overwrite)...")
    try:
        # Convert dict to DataFrame and save, ensuring no duplicates and clean format
        df = pd.DataFrame(list(imdb_cache.items()), columns=['id', 'imdb'])
        df.drop_duplicates(subset=['id'], keep='last', inplace=True)
        df.to_csv(IMDB_CACHE_FILE, index=False, encoding='utf-8')
    except IOError as e:
        print(f"âŒ Error writing to cache file {IMDB_CACHE_FILE}: {e}")

# --- æ•°æ®å¤„ç† ---
def process_movie_data(interest_data):
    subject = interest_data.get('subject', {})
    my_rating = interest_data.get('rating', {})
    douban_id = subject.get('id')
    
    # Safely extract year from pubdate
    pubdates = subject.get('pubdate', [])
    pubdate = pubdates[0] if pubdates else ''
    year_match = re.search(r'(\d{4})', pubdate)
    year = year_match.group(1) if year_match else subject.get('year')

    processed = {
        'Const': None, # Placeholder for IMDB_ID, to be filled later
        'Your Rating': my_rating.get('value', 0) if my_rating else 0,
        'Date Rated': interest_data.get('create_time', '').split(' ')[0],
        'Title': subject.get('title'),
        'URL': f"https://movie.douban.com/subject/{douban_id}/",
        'Title Type': subject.get('type', 'movie'),
        'IMDb Rating': subject.get('rating', {}).get('value', 0), # This is Douban's rating
        'Runtime (mins)': subject.get('duration_in_seconds', 0) // 60,
        'Year': year,
        'Genres': ", ".join(subject.get('genres', [])),
        'Num Votes': subject.get('rating', {}).get('count', 0), # Douban's vote count
        'Release Date': pubdate,
        'Directors': ", ".join([d['name'] for d in subject.get('directors', [])]),
        # Extra fields from the new API that might be useful
        'MyComment': interest_data.get('comment', ''),
        'DoubanID': douban_id
    }
    return processed

# --- å¼‚æ­¥ç½‘ç»œè¯·æ±‚ ---
async def fetch_movie_list_page(session, start_index, page_size=50, retries=3):
    params = {"type": "movie", "status": "done", "count": page_size, "start": start_index, "for_mobile": 1}

    for attempt in range(retries):
        await asyncio.sleep(random.uniform(1.0, 2.0))
        try:
            # The session is now initialized with headers, so we don't pass them here.
            async with session.get(LIST_API_URL, params=params, verify_ssl=False, timeout=30) as r:
                r.raise_for_status()
                return await r.json()
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            print(f"âŒ List request failed (start={start_index}, attempt {attempt + 1}/{retries}): {type(e).__name__} - {e}")
            if attempt + 1 == retries:
                print(f"âŒ Max retries reached, giving up on page start={start_index}")
                return None
            await asyncio.sleep(3) # Wait before the next retry
    return None

async def process_interest(session, interest, imdb_cache):
    processed_data = process_movie_data(interest)
    douban_id = processed_data.get('DoubanID')
    new_cache_entry = None

    if douban_id in imdb_cache:
        processed_data['Const'] = imdb_cache[douban_id]
    else:
        mobile_url = f"https://m.douban.com/movie/subject/{douban_id}/" if douban_id else None
        imdb_id = await fetch_imdb_id_from_web(session, mobile_url)
        if imdb_id:
            processed_data['Const'] = imdb_id
            if douban_id:
                imdb_cache[douban_id] = imdb_id
                new_cache_entry = (douban_id, imdb_id)
            
    return processed_data, new_cache_entry

async def main():
    start_time = time.time()
    print(f"ğŸ¬ å¼€å§‹ä¸ºç”¨æˆ· {DOUBAN_USER_ID} çˆ¬å–å·²çœ‹ç”µå½±åˆ—è¡¨...")
    output_filename = f"douban_{DOUBAN_USER_ID}_ratings.csv"
    imdb_cache = load_imdb_cache()
    print(f"âœ… å·²ä» '{IMDB_CACHE_FILE}' åŠ è½½ {len(imdb_cache)} æ¡ç¼“å­˜è®°å½•ã€‚")

    # --- å¢é‡æ›´æ–°é€»è¾‘: åŠ è½½å·²æœ‰æ•°æ® ---
    df_existing = pd.DataFrame()
    existing_ids = set()
    if os.path.exists(output_filename):
        try:
            cols = pd.read_csv(output_filename, nrows=0, encoding='utf-8-sig').columns.tolist()
            if 'DoubanID' not in cols:
                print(f"âš ï¸  '{output_filename}' is in an old format (missing 'DoubanID'). Deleting it to recreate.")
                os.remove(output_filename)
            else:
                df_existing = pd.read_csv(output_filename, dtype={'DoubanID': str}, encoding='utf-8-sig')
                if not df_existing.empty:
                    existing_ids = set(df_existing['DoubanID'].dropna().astype(str))
                    print(f"âœ… å·²ä» '{output_filename}' åŠ è½½ {len(existing_ids)} æ¡å·²æœ‰ç”µå½±è®°å½•ï¼Œå°†è¿›è¡Œå¢é‡æ›´æ–°ã€‚")
        except (pd.errors.EmptyDataError, FileNotFoundError):
            print(f"âš ï¸  '{output_filename}' å­˜åœ¨ä½†ä¸ºç©ºæˆ–æ— æ³•è¯»å–, å°†é‡æ–°åˆ›å»ºã€‚")
        except Exception as e:
            print(f"âŒ Error processing existing ratings file '{output_filename}': {e}. Will try to recreate it.")
            try: os.remove(output_filename)
            except OSError as remove_err: print(f"âŒ Failed to delete problematic file: {remove_err}")

    # Create a single session with the headers from config, to be used for all requests.
    async with aiohttp.ClientSession(headers=DOUBAN_HEADERS) as session:
        # --- Cookie Validation Step ---
        if not await validate_cookie(session):
            print("\nğŸ›‘ è¯·æ ¹æ® config.py ä¸­çš„æŒ‡å¼•æ›´æ–°æ‚¨çš„ Douban Cookie åå†è¯•ã€‚")
            return

        # --- æ™ºèƒ½å¢é‡è·å–é€»è¾‘ ---
        print("\nğŸš€ æ­¥éª¤ 1/2: æ™ºèƒ½å¢é‡è·å–æœ€æ–°çš„ç”µå½±è®°å½•...")
        new_interests = []
        should_stop_fetching = False
        page_num = 0
        page_size = 50

        with tqdm(desc="å¢é‡è·å–é¡µé¢", unit="page") as pbar:
            while not should_stop_fetching:
                page_data = await fetch_movie_list_page(session, page_num * page_size, page_size)
                pbar.update(1)

                if not page_data or 'interests' not in page_data or not page_data['interests']:
                    pbar.set_description("å·²åˆ°è¾¾æœ«é¡µ")
                    break # Reached the end of the user's ratings

                interests_on_page = page_data['interests']
                
                for interest in interests_on_page:
                    douban_id = interest.get('subject', {}).get('id')
                    if douban_id in existing_ids:
                        pbar.set_description("å‘ç°é‡å¤è®°å½•,åœæ­¢è·å–")
                        should_stop_fetching = True
                        break
                    else:
                        new_interests.append(interest)
                
                if should_stop_fetching:
                    break
                page_num += 1

        if not new_interests:
            print("\nâœ… æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
            end_time = time.time()
            print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
            return
            
        print(f"\nâœ… å‘ç° {len(new_interests)} æ¡æ–°è®°å½•ã€‚")
        
        print("\nğŸš€ æ­¥éª¤ 2/2: å¹¶å‘ä»ç¼“å­˜æˆ–ç½‘é¡µè·å–IMDB_ID (ä»…å¤„ç†æ–°ç”µå½±)...")
        processing_tasks = [process_interest(session, interest, imdb_cache) for interest in new_interests]
        new_movies_data = []
        new_cache_entries = []
        for f in tqdm(asyncio.as_completed(processing_tasks), total=len(processing_tasks), desc="è·å–IMDB ID"):
            processed_data, new_cache_entry = await f
            if processed_data:
                new_movies_data.append(processed_data)
            if new_cache_entry:
                new_cache_entries.append(new_cache_entry)

    if new_cache_entries:
        save_imdb_cache(imdb_cache)

    df_new = pd.DataFrame(new_movies_data)
    
    # --- å¢é‡æ›´æ–°é€»è¾‘: åˆå¹¶æ•°æ® ---
    df_final = pd.concat([df_new, df_existing], ignore_index=True)

    print(f"\nğŸ’¾ æ­£åœ¨å°† {len(df_final)} æ¡æ•°æ®ä¿å­˜åˆ° {output_filename}...")
    
    final_columns = ['Const', 'Your Rating', 'Date Rated', 'Title', 'URL', 'Title Type', 
                     'Douban Rating', 'Runtime (mins)', 'Year', 'Genres', 'Num Votes', 
                     'Release Date', 'Directors', 'MyComment', 'DoubanID']
    
    for col in final_columns:
        if col not in df_final.columns:
            df_final[col] = None
            
    df_final = df_final[final_columns]
    df_final.drop_duplicates(subset=['DoubanID'], keep='first', inplace=True)
    df_final.sort_values(by='Date Rated', ascending=False, inplace=True)
    
    df_final.to_csv(output_filename, index=False, encoding='utf-8-sig')

    end_time = time.time()
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print(f"æ–°å¢ {len(df_new)} æ¡è®°å½•ã€‚")
    print(f"æ€»è®°å½•æ•°: {len(df_final)}ã€‚")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"æ•°æ®å·²ä¿å­˜åœ¨: {output_filename}")


if __name__ == '__main__':
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢äº†è„šæœ¬ã€‚")
